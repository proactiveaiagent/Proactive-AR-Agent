# AR Recognition Agent

A LangChain-based multimodal agent that ingests **video + audio** (and optionally extracted frames + ASR), produces a **structured JSON scene description**, and stores/query user-specific memories in **ChromaDB**.

## Key goals
- Multimodal understanding (video frames + audio transcript / audio events)
- Output strictly conforms to a JSON schema 
- Memory is keyed by `user_id` for retrieval during inference
- Model layer is abstracted (easy to add new models later)

## Architecture
```
AR_Recognition_Agent/
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ audio2.mp3
â”‚   â”œâ”€â”€ video.mp4
â”‚   â””â”€â”€ video2.mp4
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ agent/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ ar_agent.py
â”‚   â”‚   â”œâ”€â”€ ar_matcher.py
â”‚   â”‚   â””â”€â”€ prompts.py
â”‚   â”œâ”€â”€ cli.py
â”‚   â”œâ”€â”€ memory/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ chroma_memory.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚   â”œâ”€â”€ qwen3vl.py
â”‚   â”‚   â””â”€â”€ whisper.py
â”‚   â”œâ”€â”€ pipeline/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ preprocess.py
â”‚   â”œâ”€â”€ schema/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ object_description_builder.py
â”‚   â”‚   â”œâ”€â”€ scene_schema.py
â”‚   â”‚   â””â”€â”€ schema_prompt_builder.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ chroma_visualizer.py
â”‚       â””â”€â”€ json_utils.py
â”œâ”€â”€ cli.sh
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ visual.py
```

## Environment
VRAM: at least 24GB GPU memory (e.g., A100, RTX 3090, RTX 6000, etc.) is recommended. However, the real memory usage depends on the model size and input.
* 9s 512 * 512 video + audio with Qwen3-VL-2B-Instruct(bf16) and whisper-small requires ~`12`GB VRAM.(rtx 6000, sdpa attention) 

You can also use the multi-GPUs parallelism if you have multiple GPUs available.

This repo assumes **Python 3.10+** is available (your environment has `python3`).

1. Create a new environment and install cudatoolkits:
```
conda create -n aragent python=3.10 
conda activate aragent
conda install conda-forge::ffmpeg
conda install conda-forge::av


If the `cuda-toolkit` is already installed in the environment and its version is higher than 12.1.0, please skip this step. Otherwise, run the following command to install it:


conda install nvidia::cuda-toolkit==12.1.0
```
2. Install PyTorch with CUDA support:
```
pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --extra-index-url https://download.pytorch.org/whl/cu121
```

3. Install other dependencies:
```
pip install -r requirements.txt
```

4. (Optional) If you want to use flash attention to accelerate the model and reduce memory usage, please make sure your GPUs are Ampere architecture (e.g., A100, RTX 3090, etc.), and then install flash attention:
```
pip install -U flash-attn --no-build-isolation
```
## Model setup
Please download the model checkpoint:
```
conda activate aragent

hf download Qwen/Qwen3-VL-8B-Instruct --local-dir /path/to/Qwen3-VL-8B-Instruct
model:Qwen3-VL-2/4/8B-Instruct

hf download openai/whisper-large-v3 --local-dir /path/to/whisper-large-v3
model:whisper-tiny/base/small/medium/large/large-v2/large-v3

hf download sentence-transformers/all-MiniLM-L6-v2 --local-dir /path/to/all-MiniLM-L6-v2

```

## Run (example)
* Multimodal scene understanding: 
```
bash cli.sh
```
* Memory visualization:
```
python visual.py
```

### CLI Arguments

This command performs **multimodal scene understanding** using video and audio inputs, combined with speech recognition, embedding-based memory, and structured output generation.

```bash
python -m src.cli \
    --video "./examples/video2.mp4" \
    --max_pixels 262144 \
    --audio "./examples/audio2.mp3" \
    --model "/path/to/Qwen3-VL-8B-Instruct" \
    --torch_dtype "bfloat16" \
    --attn_implementation "sdpa" \
    --user-id "test_user" \
    --persist-dir "./chroma_db" \
    --out "./outputs/scene.json" \
    --embed-model "/path/to/all-MiniLM-L6-v2" \
    --asr-model "/path/to/whisper-large-v3"
```



## Argument List

### ğŸ¥ Input Data

| Argument       | Type  | Description                                                                              |
| -------------- | ----- | ---------------------------------------------------------------------------------------- |
| `--video`      | `str` | Path to the input video file (e.g., MP4).                                                |
| `--audio`      | `str` | Path to the input audio file, used for speech recognition.(can be None)                               |
| `--max_pixels` | `int` | Maximum number of pixels per video frame (`H Ã— W â‰¤ max_pixels`) to control memory usage. |

---

### ğŸ§  Multimodal Model Configuration

| Argument                | Type  | Description                                                                                                         |
| ----------------------- | ----- | ------------------------------------------------------------------------------------------------------------------- |
| `--model`               | `str` | Path to the multimodal / vision-language model (local or vLLM output directory).                                    |
| `--torch_dtype`         | `str` | Model precision, e.g. `float16` or `bfloat16`.                                                                      |
| `--attn_implementation` | `str` | Attention backend implementation. `sdpa` enables PyTorch Scaled Dot-Product Attention for better memory efficiency.(others: eager, flash_attention_2) |

---

### ğŸ—£ï¸ Automatic Speech Recognition (ASR)

| Argument      | Type  | Description                                                               |
| ------------- | ----- | ------------------------------------------------------------------------- |
| `--asr-model` | `str` | Path to the ASR model (e.g., `whisper-large-v3`) for audio transcription. (whisper-series models are supported) |

---

### ğŸ§© Embeddings & Memory

| Argument        | Type  | Description                                                                             |
| --------------- | ----- | --------------------------------------------------------------------------------------- |
| `--embed-model` | `str` | Path to the embedding model (e.g., `all-MiniLM-L6-v2`) used for semantic vectorization. |
| `--persist-dir` | `str` | Directory for persistent storage of ChromaDB vectors.                                   |
| `--user-id`     | `str` | Unique user identifier to isolate user-specific memory spaces.                          |

---

### ğŸ“¤ Output

| Argument | Type  | Description                                                                              |
| -------- | ----- | ---------------------------------------------------------------------------------------- |
| `--out`  | `str` | Path to the output file (JSON format) containing structured scene understanding results. |


## Output Format

## 1. High-Level Structure (Top-Level Schema)

```text
SceneUnderstandingOutput
â”œâ”€ scene_narrative
â”œâ”€ location_tag
â”œâ”€ what_is_happening
â”œâ”€ spatial_environmental_analysis
â”œâ”€ detected_people_analysis
â”œâ”€ interactive_objects_detail
â”œâ”€ detected_text_in_scene
â”œâ”€ user_status
â”œâ”€ user_interactions
â”œâ”€ is_user_speaking
â”œâ”€ sound_events_detected
â”œâ”€ extra
â””â”€ stored_objects
```

**Design characteristics**

* Combines **current-frame perception** with **long-term object memory**
* Clearly separates **observation**, **inference**, and **interaction**
* Designed for **AR systems, multimodal agents, and memory-based reasoning**

---

## 2. Scene-Level Semantics

```json
scene_narrative: string
location_tag: string
what_is_happening: string
```

**Purpose**

* Natural-language summary of the scene
* High-level contextual anchor for humans and LLMs
* Suitable for narration, summarization, and embedding-based retrieval

---

## 3. Spatial & Environmental Analysis

```json
spatial_environmental_analysis: {
  user_reach_range: string
  critical_interaction_zone: string
  lighting_state: string
  noise_level_category: string
  safety_hazards: string
}
```

**Purpose**

* Describes physical accessibility and interaction feasibility
* Supports AR safety checks and interaction planning
* Abstracted understanding rather than raw sensor output

---

## 4. People & Social Context Analysis

```json
detected_people_analysis: {
  relationship_situation_summary: string
  people_list: [
    {
      role: string
      location_relative_to_user: string
      attention_target: string
      activity_state: string
    }
  ]
}
```

**Key points**

* Supports multiple people
* Encodes roles and relationships relative to the user
* Enables attention modeling and social-context reasoning

---

## 5. Interactive Objects (Current Scene)

```json
interactive_objects_detail: [
  {
    object_name: string
    object_type: string
    spatial_relation: string
    current_state: string
    affordance: string[]
    digital_connectivity: string
  }
]
```

**Purpose**

* Captures not only what objects exist, but:

  * Where they are
  * Their current state
  * What actions they afford
* Central to action planning and interaction reasoning

---

## 6. Text Detected in Scene (OCR & ASR Unified)

```json
detected_text_in_scene: [
  {
    text_content: string
    text_source_description: string
    text_role: string
    associated_object_id: string | null
    is_interactive: boolean
    ocr_confidence: string
  }
]
```

**Purpose**

* Unified abstraction for:

  * Visual text (OCR)
  * Spoken text (ASR)
* Supports UI understanding, command detection, and dialogue triggers

---

## 7. User State & Attention Modeling

```json
user_status: {
  status_inference: string
  observable_behaviors: string[]
  gaze_target: string
  gaze_duration: string
  peripheral_awareness: string[]
}
```

**Purpose**

* Infers user intent and mental state from observable behavior
* Feeds proactive agent behavior and memory storage

---

## 8. Interaction Abstraction Layer

```json
user_interactions: {
  with_surroundings: string[]
  with_ar_system: {
    common_apps: string[]
    typical_behaviors: string[]
  }
  with_agents: string[]
}
```

**Purpose**

* Describes how the user interacts with:

  * The physical environment
  * AR systems
  * Other agents
* Places the user within a broader interactive ecosystem

---

## 9. Audio Events

```json
is_user_speaking: boolean

sound_events_detected: [
  {
    event_type: string
    source_location: string
    sound_level_description: string
    asr_transcript: string
    asr_confidence: string
  }
]
```

**Purpose**

* Treats sound as time-aligned events, not static attributes
* Supports speech-aware and audio-aware agents

---

## 10. Long-Term Object Memory

```json
stored_objects: [
  {
    object_id: string
    object_name: string
    object_type: string
    spatial_relation: string
    current_state: string
    affordance: string[]
    digital_connectivity: string
    first_seen: datetime
    last_seen: datetime
    seen_count: number
  }
]
```

**Key characteristics**

* Persistent object identity (`object_id`)
* Temporal tracking (first seen, last seen, frequency)
* Enables object permanence, memory retrieval, and long-term reasoning

---

## 11. Extensibility Field

```json
extra: {}
```

**Purpose**

* Forward-compatible extension point
* Allows adding new modalities or metadata without breaking the schema


## Notes
- For large videos, sample frames (e.g., 1 fps) to control token cost.
